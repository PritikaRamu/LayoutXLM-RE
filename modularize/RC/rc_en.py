# -*- coding: utf-8 -*-
"""LayoutXLMRE HF inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16wqA3oTUf7yzUKsSSZxiMf1443_ZO3wC
"""

from transformers import PreTrainedTokenizerBase, LayoutLMv2FeatureExtractor, LayoutXLMTokenizer, AutoTokenizer
from transformers.file_utils import PaddingStrategy

import torch
from torch import nn

import eval_metrics

from dataclasses import dataclass
from typing import Dict, Tuple, Optional, Union

# from datasets import load_metric
from transformers.trainer_utils import EvalPrediction
import numpy as np

import pdb
import wandb
import preprocess_funsd

import argparse

argParser = argparse.ArgumentParser()
argParser.add_argument("--gpu", type=int, help="GPU ID")
argParser.add_argument("--run", type=int, help="run number")
argParser.add_argument("--lr", type=float, help="learning rate")
argParser.add_argument("--warm", type=float, help="warmup ratio")
argParser.add_argument("--batch", type=int, help="train batch size")
args = argParser.parse_args()

torch.cuda.set_device(args.gpu)

wandb.init(
    # set the wandb project where this run will be logged
    project="Form-IE_RC_en",
    
    # track hyperparameters and run metadata
    config={
    "learning_rate": f'{args.lr} changed upper bound to 1e-5' ,
    "architecture": "layoutXLM fine tune for RC with spatial attention with visual embeddings",
    "dataset": 'funsd (hopefully correct preproc)',
    "epochs": 200,
    "batch_size": args.batch,
    "warmup_ratio": args.warm,
    "output_dir": f"checkpoints{args.run}",
    }
)

feature_extractor = LayoutLMv2FeatureExtractor(apply_ocr=False)

@dataclass
class DataCollatorForKeyValueExtraction:
    """
    Data collator that will dynamically pad the inputs received, as well as the labels.
    Args:
        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):
            The tokenizer used for encoding the data.
        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:
            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
              sequence if provided).
            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
              maximum acceptable input length for the model if that argument is not provided.
            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
              different lengths).
        max_length (:obj:`int`, `optional`):
            Maximum length of the returned list and optionally padding length (see above).
        pad_to_multiple_of (:obj:`int`, `optional`):
            If set will pad the sequence to a multiple of the provided value.
            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
            7.5 (Volta).
        label_pad_token_id (:obj:`int`, `optional`, defaults to -100):
            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).
    """

    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    label_pad_token_id: int = -100

    def __call__(self, features):
        label_name = "label" if "label" in features[0].keys() else "labels"
        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None

        has_image_input = "image" in features[0]
        has_bbox_input = "bbox" in features[0]
        if has_image_input:
            image = feature_extractor([torch.tensor(feature["image"]) for feature in features], return_tensors="pt")['pixel_values']
            # image = ImageList.from_tensors([torch.tensor(feature["image"]) for feature in features], 32)
            for feature in features:
                del feature["image"]
        batch = self.tokenizer.pad(
            features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            # Conversion to tensors will fail if we have labels as they are not of the same length yet.
            return_tensors="pt" if labels is None else None,
        )

        if labels is None:
            return batch

        sequence_length = torch.tensor(batch["input_ids"]).shape[1]
        padding_side = self.tokenizer.padding_side
        if padding_side == "right":
            batch["labels"] = [label + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels]
            if has_bbox_input:
                batch["bbox"] = [bbox + [[0, 0, 0, 0]] * (sequence_length - len(bbox)) for bbox in batch["bbox"]]
        else:
            batch["labels"] = [[self.label_pad_token_id] * (sequence_length - len(label)) + label for label in labels]
            if has_bbox_input:
                batch["bbox"] = [[[0, 0, 0, 0]] * (sequence_length - len(bbox)) + bbox for bbox in batch["bbox"]]

        batch = {k: torch.tensor(v, dtype=torch.int64) if isinstance(v[0], list) else v for k, v in batch.items()}
        if has_image_input:
            batch["image"] = image
        return batch

""" Define model evaluation script"""
def model_eval(model, test_dataloader):
    model.eval()
    label_names = ['labels', 'relations']
    metric_key_prefix = 'eval'
    re_labels = None
    pred_relations = None
    entities = None
    for batch in test_dataloader:
        with torch.no_grad():
            temp = batch['id']
            del batch['id']
            #del batch['hd_image']
            for k, v in batch.items():
                if hasattr(v, "to") and hasattr(v, "device"):
                    batch[k] = v.to(device)

            # forward pass
            outputs = model(**batch)
            labels = tuple(batch.get(name) for name in label_names)


            # Setup labels
            re_labels = labels[1] if re_labels is None else re_labels + labels[1]
            pred_relations = (
                outputs.pred_relations if pred_relations is None else pred_relations + outputs.pred_relations
            )
            entities = outputs.entities if entities is None else entities + outputs.entities        
            
    gt_relations = []

    for b in range(len(re_labels)):
        rel_sent = []
        for head, tail in zip(re_labels[b]["head"], re_labels[b]["tail"]):
            try:
                rel = {}
                rel["head_id"] = head
                rel["head"] = (entities[b]["start"][rel["head_id"]], entities[b]["end"][rel["head_id"]])
                rel["head_type"] = entities[b]["label"][rel["head_id"]]

                rel["tail_id"] = tail
                rel["tail"] = (entities[b]["start"][rel["tail_id"]], entities[b]["end"][rel["tail_id"]])
                rel["tail_type"] = entities[b]["label"][rel["tail_id"]]

                rel["type"] = 1

                rel_sent.append(rel)
            except:
                continue

        gt_relations.append(rel_sent)

    re_metrics = eval_metrics.compute_metrics(EvalPrediction(predictions=pred_relations, label_ids=gt_relations))

    re_metrics = {
        "precision": re_metrics["ALL"]["p"],
        "recall": re_metrics["ALL"]["r"],
        "f1": re_metrics["ALL"]["f1"],
    }
    re_metrics[f"{metric_key_prefix}_loss"] = outputs.loss.mean().item()

    metrics = {}

    # # Prefix all keys with metric_key_prefix + '_'
    for key in list(re_metrics.keys()):
        if not key.startswith(f"{metric_key_prefix}_"):
            metrics[f"{metric_key_prefix}_{key}"] = re_metrics.pop(key)
        else:
            metrics[f"{key}"] = re_metrics.pop(key)

    print(metrics)

    return metrics


"""Load dataset, using huggingface datasets"""

dataset = {}

train_anno_path = '/home/pritika/annotations'
train_image_path = '/home/pritika/images'

test_anno_path = '/home/pritika/testing_data/annotations'
test_image_path = '/home/pritika/testing_data/images'

dataset['train'] = preprocess_funsd.create_dataset(train_anno_path,train_image_path)
dataset['validation'] = preprocess_funsd.create_dataset(test_anno_path, test_image_path)

"""Create our tokenizer"""

tokenizer = AutoTokenizer.from_pretrained('/home/pritika/layoutXLM/layoutxlm-base', pad_token='<pad>')

"""Let's setup the dataloaders"""

from torch.utils.data import DataLoader
data_collator = DataCollatorForKeyValueExtraction(
    tokenizer,
    pad_to_multiple_of=8,
    padding=True,
    max_length=512,
)

train_dataset = dataset['train']
test_dataset = dataset['validation']

train_batch_size = args.batch
test_batch_size = 2

train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, collate_fn=data_collator, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, collate_fn=data_collator)

file_list = train_dataset['id']
file_list.sort()
"""And set our device"""

device = 'cuda'


"""And let's setup our new model"""

from RE import LayoutLMv2ForRelationExtraction
model = LayoutLMv2ForRelationExtraction.from_pretrained('/home/pritika/layoutXLM/layoutxlm-base')
model.to(device)

"""Now lets do some training, first setup the optimizer"""

from transformers import AdamW

lr_bounds = (args.lr, 1e-5)
optimizer = AdamW(model.parameters(), lr=lr_bounds[0])

global_step = 0
num_train_epochs = 200
t_total = len(train_dataloader) * num_train_epochs # total number of training steps
lr_warmup_ratio = args.warm # percentage of steps we want to warm up on
lr_warmup_steps = t_total*lr_warmup_ratio

print(f'Hyperparameters:\ntrain_batch_size  = {train_batch_size}\ntest_batch_size = {test_batch_size}\nlr = {lr_bounds[0]}\nlr_warmup_ratio = {lr_warmup_ratio}')
print(f'total steps: {t_total}, total examples:, {t_total*train_batch_size}, warmup steps: {lr_warmup_steps}')

max_f1 = -1

#put the model in training mode
model.train()
for epoch in range(num_train_epochs):  
    print(f'Epoch: {epoch}')
    avg_loss = 0.
    nb_examples = 0
    for batch in train_dataloader:
        temp = batch['id']

        del batch['id']
        #del batch['hd_image']
        for k, v in batch.items():
            if hasattr(v, "to") and hasattr(v, "device"):
                batch[k] = v.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(**batch) 
        loss = outputs.loss
        avg_loss += loss.item()
        nb_examples += batch['input_ids'].shape[0]
        
        # print loss every 10 steps
        if global_step % 10 == 0:
          print(f'Loss after {global_step} steps: {avg_loss/ nb_examples}')

        loss.backward()
        optimizer.step()
        global_step += 1

        # Do some learning rate warm up
        if global_step < lr_warmup_steps:
          for g in optimizer.param_groups:
              g['lr'] += (lr_bounds[1]-lr_bounds[0]) / lr_warmup_steps

    print(f'epoch done, avg_loss: {avg_loss/nb_examples}, nb_examples: {nb_examples}')
    wandb.log({"loss": avg_loss/nb_examples})

    metrics = model_eval(model, test_dataloader)
    wandb.log({'f1':metrics['eval_f1']})
    if metrics['eval_f1']>=max_f1:
        max_f1 = metrics['eval_f1']
        model.save_pretrained(f'checkpoints{args.run}/checkpt-{epoch}')